# Install necessary packages
!pip install dask[complete] keras-tuner --quiet

import dask.dataframe as dd
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import keras_tuner as kt

# Load data using Dask
ddf = dd.read_csv('your_dataset.csv')  # Replace with your dataset path

# Convert 'Close (Rs.)' to numeric and forward fill NaNs
ddf['Close (Rs.)'] = dd.to_numeric(ddf['Close (Rs.)'], errors='coerce')
ddf['Close (Rs.)'] = ddf['Close (Rs.)'].fillna(method='ffill')

# Compute technical indicators
ddf = ddf.groupby('Symbol').apply(lambda df: df.assign(
    SMA_10=df['Close (Rs.)'].rolling(window=10).mean(),
    EMA_10=df['Close (Rs.)'].ewm(span=10, adjust=False).mean(),
    Momentum=df['Close (Rs.)'] - df['Close (Rs.)'].shift(10),
    Volatility=df['Close (Rs.)'].rolling(window=10).std()
)).reset_index(drop=True)

# Drop rows with NaN values
ddf = ddf.dropna()

# Compute the Dask DataFrame to get a pandas DataFrame
final_df = ddf.compute()

# Feature and target columns
feature_cols = ['SMA_10', 'EMA_10', 'Momentum', 'Volatility']
target_col = 'Close (Rs.)'

# Scale features
feature_scaler = MinMaxScaler()
final_df[feature_cols] = feature_scaler.fit_transform(final_df[feature_cols])

# Scale target
target_scaler = MinMaxScaler()
final_df[target_col] = target_scaler.fit_transform(final_df[[target_col]])

# One-hot encode 'Symbol' column if it exists
if 'Symbol' in final_df.columns:
    final_df = pd.get_dummies(final_df, columns=['Symbol'], drop_first=True)

# Create sequences
def create_sequences(df, feature_cols, target_col, seq_length):
    X, y = [], []
    for i in range(len(df) - seq_length):
        X.append(df[feature_cols].iloc[i:i+seq_length].values)
        y.append(df[target_col].iloc[i+seq_length])
    return np.array(X), np.array(y)

seq_length = 60
X, y = create_sequences(final_df, feature_cols, target_col, seq_length)

# Train-test split
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# Build model function for Keras Tuner
def build_model(hp):
    model = Sequential()
    model.add(Bidirectional(LSTM(units=hp.Int('units_layer1', min_value=32, max_value=256, step=32),
                                 return_sequences=True,
                                 activation='tanh'),
                            input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Bidirectional(LSTM(units=hp.Int('units_layer2', min_value=32, max_value=256, step=32),
                                 return_sequences=False,
                                 activation='tanh')))
    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)))
    model.add(Dense(1))

    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
                  loss=Huber(),
                  metrics=['mae'])
    return model

# Hyperparameter tuning with Keras Tuner
tuner = kt.RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=30,
    executions_per_trial=1,
    directory='keras_tuner_dir',
    project_name='lstm_tuning'
)

tuner.search(X_train, y_train,
             epochs=50,
             validation_data=(X_val, y_val),
             callbacks=[
                 tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
             ])

best_model = tuner.get_best_models(num_models=1)[0]

# Evaluate the model
y_pred_scaled = best_model.predict(X_val)
y_val_actual = target_scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()
y_pred_actual = target_scaler.inverse_transform(y_pred_scaled).flatten()

mse = mean_squared_error(y_val_actual, y_pred_actual)
mae = mean_absolute_error(y_val_actual, y_pred_actual)
rmse = np.sqrt(mse)
r2 = r2_score(y_val_actual, y_pred_actual)

print(f"\nEvaluation on Validation Set:")
print(f"MSE: {mse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R2 Score: {r2:.4f}")

# Save the best model
best_model.save('best_lstm_model.h5')
print("Trained model saved as 'best_lstm_model.h5'.")
